{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0JFgOA0pc2W"
      },
      "source": [
        "# YOLO11 Training and RKNN Conversion\n",
        "\n",
        "![frogforce503_logo_2.png](https://frogforce503.org/FFmods/img/frogforce503_logo_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dePvazaipc2h"
      },
      "source": [
        "This notebook trains a YOLO11 model using a Roboflow Dataset and converts it to the RKNN format to run on Rockchip Processors such as the RK3588 found on the Single Board Computer, the Orange Pi 5.\n",
        "\n",
        "To begin, go to session options on the right panel of the editor and select GPU T4 x2 as your accelerator, and enable using the internet.\n",
        "\n",
        "You can also upload your own pre-trained YOLO11 model if you trained it using the airockchip/ultralytics_yolo11 repository rather than training a new model.\n",
        "\n",
        "If you are new to python, object detection, jupyter notebooks, or anything else, check out the helpful links below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpwwX9BVpc2j"
      },
      "source": [
        "# Helpful Links 😀\n",
        "Jupyter Notebook Programming:\n",
        "*   Jupyter - [Jupyter Notebook Documentation](https://jupyter-notebook.readthedocs.io/en/latest/)\n",
        "*   Kaggle - [How to use Kaggle](https://www.kaggle.com/docs/notebooks)\n",
        "*   w3schools - [Python Course](https://www.w3schools.com/python/default.asp)\n",
        "*   Amazon Web Services - [Command Line Interface](https://aws.amazon.com/what-is/cli)\n",
        "*   Tutorialspoint - [Magic Commands](https://www.tutorialspoint.com/jupyter/ipython_magic_commands.htm)\n",
        "\n",
        "\n",
        "Machine Learning Fundamentals:\n",
        "*   IBM - [What is Machine Learning?](https://www.ibm.com/topics/machine-learning)\n",
        "*   IBM - [Neural Networks](https://www.ibm.com/topics/neural-networks)\n",
        "\n",
        "\n",
        "About YOLO11:\n",
        "*   Data Scientist - [What is YOLO?](https://datascientest.com/en/you-only-look-once-yolo-what-is-it)\n",
        "*   Roboflow - [What is YOLO11?](https://blog.roboflow.com/what-is-yolo11)\n",
        "*   Ultralytics - [YOLO Docs](https://docs.ultralytics.com/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "About the Orange Pi 5:\n",
        "\n",
        "*   BAE Systems - [Single Board Computers](https://www.baesystems.com/en-us/definition/what-are-single-board-computers)\n",
        "*   Orange Pi - [Orange Pi 5 Specs](http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-5.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YaA5bmipc2m"
      },
      "source": [
        "# Setup\n",
        "YOLO11 is a version of the YOLO(You Only Look Once) computer vision model created by Ultralytics. This notebook will create and train a YOLO11 model from scratch.\n",
        "\n",
        "We are going through a non-standard installation process by cloning the 3rd party ultralytics_yolo11 repository which implements RKNN support in the YOLO11 model.\n",
        "\n",
        "The official ultralytics YOLO11 model does not have RKNN support, so we can't install the standard stuff.\n",
        "\n",
        "This is important because in order for the NPUs the Rockchip Processors to run the model, they have to be formatted in RKNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7ux4OrJpc2p",
        "outputId": "939dff23-ebb5-4606-aa0b-c90dbab010e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content\n",
            "fatal: destination path 'ultralytics' already exists and is not an empty directory.\n",
            "/content/ultralytics\n",
            "Obtaining file:///content/ultralytics\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics==8.3.9) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.9) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.9) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.9) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.9) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.9) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.9) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics==8.3.9) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.3.9) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics==8.3.9) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.9) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.9) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.9) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics==8.3.9) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics==8.3.9) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics==8.3.9) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics==8.3.9) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics==8.3.9) (3.0.2)\n",
            "Building wheels for collected packages: ultralytics\n",
            "  Building editable for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ultralytics: filename=ultralytics-8.3.9-0.editable-py3-none-any.whl size=22989 sha256=634722f04b367568594136e1ba61693b047354a8f8a887e0b3473c1b291af69f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zp059a7l/wheels/ea/71/6b/a9012dfb148489fd8125c2310e565414c996b3c7721defe799\n",
            "Successfully built ultralytics\n",
            "Installing collected packages: ultralytics\n",
            "  Attempting uninstall: ultralytics\n",
            "    Found existing installation: ultralytics 8.3.9\n",
            "    Uninstalling ultralytics-8.3.9:\n",
            "      Successfully uninstalled ultralytics-8.3.9\n",
            "Successfully installed ultralytics-8.3.9\n"
          ]
        }
      ],
      "source": [
        "import os #python module that allows for you to interact with the elements of the operating system like directories, processes, environments, paths\n",
        "\n",
        "#getcwd() gets the working directory we are currently in. We're always working in this directory, we'll store it in avariable called root path\n",
        "#os.getcwd returns a string\n",
        "root_path = os.getcwd()\n",
        "\n",
        "#in a kaggle environment, it should print /kaggle/working\n",
        "print(root_path)\n",
        "\n",
        "#cd stands for change directory, we are just moving into the root path directory\n",
        "#% means we are doing a magic command, % means it works on one line, %% means it works on an entire cell of code. It is used in interactive environments like Jupyter notebooks such as this one.\n",
        "%cd {root_path}\n",
        "\n",
        "#! means a shell command which is in command line interface.\n",
        "!git clone https://github.com/airockchip/ultralytics_yolo11 ultralytics\n",
        "%cd ultralytics\n",
        "\n",
        "#installing dependencies\n",
        "!pip install -e .\n",
        "\n",
        "#import to use for code\n",
        "import ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpXEdBCdpc2t"
      },
      "source": [
        "# Downloading a Dataset\n",
        "[Roboflow Universe](https://universe.roboflow.com/) is an open source repository where you can find datasets to train models you need.\n",
        "\n",
        "With object detection, a dataset is a collection of images that are annotated with bounding boxes and classifications of objects your machine learning model has to detect.\n",
        "\n",
        "For this notebook's use, we want to use a YOLO11 object detection model, that means we need to download a dataset from Roboflow that is formatted to be used by the model.\n",
        "\n",
        "To get a dataset, go to https://universe.roboflow.com/ and find a project. Press datasets on the left bar of the page. Then on the right side, press download dataset. Select a format, for this notebook, select YOLO11 and select show download code. Copy and paste the code snippet in the cell below. You NEED to do this NO MATTER WHAT because each user has their own roboflow API key.\n",
        "\n",
        "Even if you aren't copying a code snippet, you still need to find your API Key. Go to https://app.roboflow.com/ and log in. In the left navigation bar, press settings. Then press API Keys. You will use your private API Key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktRjFvq0sJUt"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read(root_path + '/config.ini')\n",
        "ROBOFLOW_API_KEY = config.get('ROBOFLOW', 'ROBOFLOW_API_KEY')\n",
        "ROBOFLOW_WORKSPACE = config.get('ROBOFLOW', 'ROBOFLOW_WORKSPACE')\n",
        "ROBOFLOW_PROJECT = config.get('ROBOFLOW', 'ROBOFLOW_PROJECT')\n",
        "ROBOFLOW_PROJECT_VERSION = config.get('ROBOFLOW', 'ROBOFLOW_PROJECT_VERSION')\n",
        "ROBOFLOW_PROJECT_VERSION_DOWNLOAD = config.get('ROBOFLOW', 'ROBOFLOW_PROJECT_VERSION_DOWNLOAD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRkyobU1pc2y",
        "outputId": "d7f7dc56-c6d9-41c2-c66e-20f09ed4a7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.61)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n",
            "Requirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.22.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Crime-Detection-1 to yolov11:: 100%|██████████| 832633/832633 [00:13<00:00, 63054.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Crime-Detection-1 in yolov11:: 100%|██████████| 27886/27886 [00:05<00:00, 5204.14it/s] \n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
        "project = rf.workspace(ROBOFLOW_WORKSPACE).project(ROBOFLOW_PROJECT)\n",
        "version = project.version(ROBOFLOW_PROJECT_VERSION)\n",
        "dataset = version.download(ROBOFLOW_PROJECT_VERSION_DOWNLOAD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daVBZsehpc23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f72026e-f3c7-4485-8f80-099aabae8d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: ../train/images\n",
            "val: ../valid/images\n",
            "test: ../test/images\n",
            "\n",
            "nc: 2\n",
            "names: ['knife', 'pistol']\n",
            "\n",
            "roboflow:\n",
            "  workspace: proyecto-final-6hd5x\n",
            "  project: crime-detection-2upma\n",
            "  version: 1\n",
            "  license: CC BY 4.0\n",
            "  url: https://universe.roboflow.com/proyecto-final-6hd5x/crime-detection-2upma/dataset/1"
          ]
        }
      ],
      "source": [
        "#%cat prints the data in a file (referenced by its path)\n",
        "%cat {dataset.location}/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXJsOwXpc26"
      },
      "source": [
        "### Fixing Dataset Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3K8RMdapc2-"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream: #open the data.yaml file, 'r' means it is read only\n",
        "    data = yaml.safe_load(stream) #takes the data from yaml and turns it into dictionaries\n",
        "    data[\"train\"] = dataset.location + \"/train/images\" if \"train\" in data else print(\"dataset doesn't have training images\") #the key 'train' contains the path to the train images, replacing existing path with the correct path\n",
        "    data[\"val\"] = dataset.location + \"/valid/images\" if \"val\" in data else print(\"dataset doesn't have validating images\")\n",
        "    data[\"test\"] = dataset.location + \"/test/images\" if \"test\" in data else print(\"dataset doesn't have testing images\")\n",
        "\n",
        "with open(dataset.location + \"/data.yaml\", 'w') as stream: #'w' means we can write in the file\n",
        "    yaml.dump(data, stream, default_flow_style=False) #writes the data variable into the data.yaml file, default flow style just changes how the data looks in the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvjRecXnpc3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcd1338-7627-4dfc-b15f-46891c01c156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "names:\n",
            "- knife\n",
            "- pistol\n",
            "nc: 2\n",
            "roboflow:\n",
            "  license: CC BY 4.0\n",
            "  project: crime-detection-2upma\n",
            "  url: https://universe.roboflow.com/proyecto-final-6hd5x/crime-detection-2upma/dataset/1\n",
            "  version: 1\n",
            "  workspace: proyecto-final-6hd5x\n",
            "test: /content/ultralytics/Crime-Detection-1/test/images\n",
            "train: /content/ultralytics/Crime-Detection-1/train/images\n",
            "val: /content/ultralytics/Crime-Detection-1/valid/images\n"
          ]
        }
      ],
      "source": [
        "%cat {dataset.location}/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQDXvjaopc3B"
      },
      "source": [
        "# Creating a Model\n",
        "\n",
        "If you have already trained a model using the airockchip/ultralytics_yolo11 repository, and you want to skip the training steps set the boolean `skip_training` variable equal to True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoxncInypc3C"
      },
      "outputs": [],
      "source": [
        "skip_training = False\n",
        "\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def skip(line, cell):\n",
        "    if not eval(line):\n",
        "        return get_ipython().run_cell(cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWA3imDrpc3C"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P3O5l6fpc3D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "#register_line_cell_magic means we can write our own magic command\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell): #takes EVERYTHING in the cell and puts it in the file that's on the same line as when write template is called\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))\n",
        "        #** means unspecified number of arguments.\n",
        "        #globals() returns the global symbol table which has the every variable in the notebook.\n",
        "        #Any variables inside the cell are replaced by their values from the global symbol table using the format function.\n",
        "        #write writes the entire formatted cell into the file, including comments\n",
        "        print(\"Wrote successfully to \" + line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLNCsmflpc3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676f8a15-319f-4fb9-8e31-110a4a7588ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_classes: 2\n",
            "/content/ultralytics\n"
          ]
        }
      ],
      "source": [
        "# define number of classes based on YAML\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream: #reads the data.yaml file as the variable stream\n",
        "    #yaml.safeload returns the contents of stream as a dictionary of dictionaries\n",
        "    num_classes = str(len(yaml.safe_load(stream)['names'])) #number of terms in the \"names\" key is the number of classes\n",
        "\n",
        "print(f\"num_classes: {num_classes}\")\n",
        "%cd {root_path}/ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BB2ewR6pc3F"
      },
      "source": [
        "### YOLO11 Architecture\n",
        "Yolo11 architecture found here: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/11/yolo11.yaml.\n",
        "\n",
        "Creating a custom yaml file to fit the number of classes in our dataset so that we can train models from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixVvyfjepc3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336c7e73-f08c-4f9c-a43d-6270d371970a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote successfully to custom_yolo11.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writetemplate custom_yolo11.yaml\n",
        "\n",
        "# Ultralytics YOLO 🚀, AGPL-3.0 license\n",
        "# YOLO11 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect\n",
        "\n",
        "# Parameters\n",
        "nc: {num_classes}  # number of classes\n",
        "scales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\n",
        "  # [depth, width, max_channels]\n",
        "  n: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs\n",
        "  s: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\n",
        "  m: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs\n",
        "  l: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs\n",
        "  x: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs\n",
        "\n",
        "# YOLO11n backbone\n",
        "backbone:\n",
        "  # [from, repeats, module, args]\n",
        "  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n",
        "  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n",
        "  - [-1, 2, C3k2, [256, False, 0.25]]\n",
        "  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n",
        "  - [-1, 2, C3k2, [512, False, 0.25]]\n",
        "  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n",
        "  - [-1, 2, C3k2, [512, True]]\n",
        "  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
        "  - [-1, 2, C3k2, [1024, True]]\n",
        "  - [-1, 1, SPPF, [1024, 5]] # 9\n",
        "  - [-1, 2, C2PSA, [1024]] # 10\n",
        "\n",
        "# YOLO11n head\n",
        "head:\n",
        "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
        "  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n",
        "  - [-1, 2, C3k2, [512, False]] # 13\n",
        "\n",
        "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
        "  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n",
        "  - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small)\n",
        "\n",
        "  - [-1, 1, Conv, [256, 3, 2]]\n",
        "  - [[-1, 13], 1, Concat, [1]] # cat head P4\n",
        "  - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium)\n",
        "\n",
        "  - [-1, 1, Conv, [512, 3, 2]]\n",
        "  - [[-1, 10], 1, Concat, [1]] # cat head P5\n",
        "  - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large)\n",
        "\n",
        "  - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H26rSkHApc3M"
      },
      "source": [
        "## Training\n",
        "You can adjust the following settings:\n",
        "\n",
        "1.   model: one of [yolo11n, yolo11s, yolo11m, yolo11l, yolo11x], yolo11n is recommended for a Orange Pi 5.\n",
        "2.   image_size: The input size of the images fed to the model. Should be a multiple of 32.\n",
        "4.   epochs: How many times the model goes through ALL the data\n",
        "\n",
        "View other arguments for the train method here: https://docs.ultralytics.com/modes/train/#train-settings\n",
        "\n",
        "You can see the progress and metrics in the console below the cell. Your cls_loss, box_loss, and dfl_loss should be minimized (less than 1) and your P (precision), R (recall), mAP50, and mAP50-95 should be maximized (as close to 1.00 as possible)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV5frNyQpc3O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df418e36-9fd8-40cc-de6e-38a885ec0329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ultralytics\n"
          ]
        }
      ],
      "source": [
        "%cd {root_path}/ultralytics\n",
        "\n",
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "image_size = 448 #640\n",
        "model = \"yolo11n\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "x-7xzEkO9fqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MPjSl96pc3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8abb21-151e-4bf2-ee15-54684198cd0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New https://pypi.org/project/ultralytics/8.3.110 available 😃 Update with 'pip install -U ultralytics'\n",
            "Ultralytics 8.3.9 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=./custom_yolo11n.yaml, data=/content/ultralytics/Crime-Detection-1/data.yaml, epochs=20, time=None, patience=100, batch=200, imgsz=448, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=rknn, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/content/ultralytics/runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 27.0MB/s]\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744920785.438016    1452 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744920785.499916    1452 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    431062  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "custom_YOLO11n summary: 319 layers, 2,590,230 parameters, 2,590,214 gradients, 6.4 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/ultralytics/runs/detect/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLO11n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n",
            "100% 5.35M/5.35M [00:00<00:00, 122MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/ultralytics/Crime-Detection-1/train/labels... 13347 images, 2 backgrounds, 0 corrupt: 100% 13347/13347 [00:05<00:00, 2541.77it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/ultralytics/Crime-Detection-1/train/labels.cache\n",
            "/content/ultralytics/ultralytics/data/augment.py:1850: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=75, p=0.0),\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/ultralytics/Crime-Detection-1/valid/labels... 295 images, 0 backgrounds, 0 corrupt: 100% 295/295 [00:00<00:00, 1681.45it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/ultralytics/Crime-Detection-1/valid/labels.cache\n",
            "Plotting labels to /content/ultralytics/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0015625), 87 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 448 train, 448 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/ultralytics/runs/detect/train\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/20      13.8G      3.225      3.813      3.971        340        448: 100% 67/67 [02:12<00:00,  1.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:06<00:00,  6.95s/it]\n",
            "                   all        295        331    0.00225      0.632     0.0391     0.0134\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/20      13.4G      2.358      3.145      2.813        331        448: 100% 67/67 [02:16<00:00,  2.04s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.97s/it]\n",
            "                   all        295        331      0.591      0.113     0.0628     0.0159\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/20      13.3G      1.877      2.554       2.22        317        448: 100% 67/67 [02:13<00:00,  1.99s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.90s/it]\n",
            "                   all        295        331      0.602     0.0958      0.066     0.0312\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/20      13.4G      1.691      2.265      2.016        309        448: 100% 67/67 [02:12<00:00,  1.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.85s/it]\n",
            "                   all        295        331      0.694      0.177      0.178     0.0946\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/20      13.4G       1.56      2.047      1.886        334        448: 100% 67/67 [02:14<00:00,  2.00s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:03<00:00,  3.06s/it]\n",
            "                   all        295        331      0.363      0.274      0.258       0.14\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/20      13.4G      1.498      1.923      1.826        289        448: 100% 67/67 [02:11<00:00,  1.97s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:04<00:00,  4.29s/it]\n",
            "                   all        295        331      0.388      0.367       0.35      0.185\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/20      13.5G      1.426      1.809      1.755        335        448: 100% 67/67 [02:16<00:00,  2.04s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.97s/it]\n",
            "                   all        295        331      0.285      0.351      0.288      0.163\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/20      13.4G      1.399       1.75      1.724        340        448: 100% 67/67 [02:16<00:00,  2.04s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.84s/it]\n",
            "                   all        295        331        0.4      0.369      0.354      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/20      13.4G      1.347      1.665      1.674        334        448: 100% 67/67 [02:14<00:00,  2.01s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:03<00:00,  3.72s/it]\n",
            "                   all        295        331      0.586      0.381      0.421      0.225\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/20      13.5G      1.333      1.638      1.661        336        448: 100% 67/67 [02:12<00:00,  1.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:03<00:00,  3.42s/it]\n",
            "                   all        295        331      0.382       0.37      0.349       0.19\n",
            "Closing dataloader mosaic\n",
            "/content/ultralytics/ultralytics/data/augment.py:1850: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=75, p=0.0),\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/20      13.3G      1.361      1.563       1.72        172        448: 100% 67/67 [02:15<00:00,  2.02s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.75s/it]\n",
            "                   all        295        331      0.617       0.37      0.458      0.257\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/20      13.2G      1.289      1.423      1.664        162        448: 100% 67/67 [02:07<00:00,  1.91s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.75s/it]\n",
            "                   all        295        331      0.587      0.469      0.518      0.293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/20      13.3G      1.247      1.349      1.621        156        448: 100% 67/67 [02:06<00:00,  1.88s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:03<00:00,  3.18s/it]\n",
            "                   all        295        331      0.643      0.518      0.579      0.306\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/20      13.2G      1.227      1.289      1.595        166        448: 100% 67/67 [02:08<00:00,  1.92s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:03<00:00,  3.13s/it]\n",
            "                   all        295        331      0.646      0.544      0.593      0.328\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/20      13.4G      1.198      1.228      1.579        156        448: 100% 67/67 [02:07<00:00,  1.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.83s/it]\n",
            "                   all        295        331      0.722       0.51      0.597       0.36\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/20      13.5G      1.167      1.187      1.549        158        448: 100% 67/67 [02:06<00:00,  1.89s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:03<00:00,  3.92s/it]\n",
            "                   all        295        331      0.704      0.561      0.648      0.381\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/20      13.4G      1.149      1.135      1.525        157        448: 100% 67/67 [02:06<00:00,  1.89s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.63s/it]\n",
            "                   all        295        331      0.701      0.591      0.667      0.379\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/20      13.3G      1.129      1.099      1.509        167        448: 100% 67/67 [02:07<00:00,  1.90s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.54s/it]\n",
            "                   all        295        331      0.744      0.586      0.679      0.407\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/20      13.3G      1.112      1.063      1.489        166        448: 100% 67/67 [02:05<00:00,  1.87s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.65s/it]\n",
            "                   all        295        331      0.749      0.617      0.707       0.42\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/20      13.2G      1.091      1.034      1.478        158        448: 100% 67/67 [02:06<00:00,  1.89s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.21s/it]\n",
            "                   all        295        331      0.731      0.623      0.695       0.42\n",
            "\n",
            "20 epochs completed in 0.764 hours.\n",
            "Optimizer stripped from /content/ultralytics/runs/detect/train/weights/last.pt, 5.4MB\n",
            "Optimizer stripped from /content/ultralytics/runs/detect/train/weights/best.pt, 5.4MB\n",
            "\n",
            "Validating /content/ultralytics/runs/detect/train/weights/best.pt...\n",
            "WARNING ⚠️ validating an untrained model YAML will result in 0 mAP.\n",
            "Ultralytics 8.3.9 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "custom_YOLO11n summary (fused): 238 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:02<00:00,  2.71s/it]\n",
            "                   all        295        331      0.748      0.615      0.707      0.421\n",
            "                 knife         90         91      0.723      0.632        0.7      0.388\n",
            "                pistol        205        240      0.774      0.598      0.714      0.454\n",
            "Speed: 0.1ms preprocess, 1.2ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/detect/train\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr/pg0 ▃▆██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr/pg1 ▃▆██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr/pg2 ▃▆██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP50(B) ▁▁▁▂▃▄▄▄▅▄▅▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     metrics/mAP50-95(B) ▁▁▁▂▃▄▄▄▅▄▅▆▆▆▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision(B) ▁▇▇▇▄▅▄▅▆▅▇▆▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall(B) █▁▁▂▃▅▄▅▅▅▅▆▇▇▆▇▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            model/GFLOPs ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        model/parameters ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: model/speed_PyTorch(ms) ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/box_loss █▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/cls_loss █▆▅▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/dfl_loss █▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/box_loss ██▆▄▃▃▃▃▂▃▂▂▂▂▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/cls_loss ▄▅█▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/dfl_loss █▇▅▄▃▃▃▂▂▃▂▂▂▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr/pg0 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr/pg1 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr/pg2 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP50(B) 0.70708\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     metrics/mAP50-95(B) 0.42071\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/precision(B) 0.74837\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       metrics/recall(B) 0.61473\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            model/GFLOPs 6.442\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        model/parameters 2590230\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: model/speed_PyTorch(ms) 1.708\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/box_loss 1.09122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/cls_loss 1.03351\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/dfl_loss 1.47769\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/box_loss 1.37006\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/cls_loss 1.21488\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val/dfl_loss 1.63527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/ultralytics/wandb/offline-run-20250417_202150-i97mupnz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20250417_202150-i97mupnz/logs\u001b[0m\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExecutionResult object at 79ca042b3950, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 79ca042b04d0, raw_cell=\"\n",
              "!yolo task=detect mode=train model=./custom_{mode..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "%%skip skip_training\n",
        "\n",
        "!yolo task=detect mode=train model=./custom_{model}.yaml data={dataset.location}/data.yaml epochs=20 imgsz={image_size} device=cuda:0 batch = 200\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC3zHGfFpc3T"
      },
      "source": [
        "## Locating trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vTe--pIpc3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9d6522-fc55-4211-d1fd-c0876839b9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ultralytics/runs/detect/train/weights/best.pt\n"
          ]
        }
      ],
      "source": [
        "latest_modified_time = 0\n",
        "latest = None\n",
        "\n",
        "#gets the path of the trained model by going through all of folder, subfolders, and files, and finding the file: best.pt\n",
        "for foldername, subfolders, filenames in os.walk(root_path):\n",
        "    for filename in filenames:\n",
        "        if filename == \"best.pt\":\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            modified_time = os.path.getmtime(file_path)\n",
        "            if modified_time > latest_modified_time:\n",
        "                latest_modified_time = modified_time\n",
        "                latest = file_path\n",
        "print(latest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI8Wv-kJpc3U"
      },
      "source": [
        "## Model Name\n",
        "\n",
        "If you are using an rknn model with PhotonVision for FRC, your model name should be {name}-{horizontal_resolution}-{vertical_resolution}-{model_type}. Name should NOT have dashes, model_type is yolov11n, yolov11s, yolov11m, etc.\n",
        "\n",
        "Modify the `model_name` variable to match your desired name, DO NOT PUT '.pt or .rknn' in the name\n",
        "\n",
        "If you leave `model_name` equal to **None**, still run the code cell below. The code will automatically generate a name that fits model naming criteria PhotonVision's criteria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19K6c5nWpc3U"
      },
      "outputs": [],
      "source": [
        "model_name = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZbjpB7bpc3U"
      },
      "source": [
        "## Moving and Renaming Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uzxx-EKpc3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9796c78-a93b-4169-bfac-35da72c0b555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Crime_Detection_1-448-448-yolov11n.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExecutionResult object at 79ca0615ed90, execution_count=None error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 79ca06a9de90, raw_cell=\"import os\n",
              "\n",
              "if model_name == None:\n",
              "    dataset_name..\" store_history=False silent=False shell_futures=True cell_id=None> result=None>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "%%skip skip_training\n",
        "import os\n",
        "\n",
        "if model_name == None:\n",
        "    dataset_name = dataset.location.replace(f\"{root_path}/ultralytics/\", \"\").replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "    model_name = f\"{dataset_name}-{image_size}-{image_size}-{model[:4] + 'v' + model[4:]}\" #yolo11n -> yolov11n\n",
        "\n",
        "if latest == None:\n",
        "    latest = f\"{root_path}/{model_name}.pt\"\n",
        "    os.rename(latest, f\"{root_path}/{model_name}.pt\")\n",
        "    latest = f\"{root_path}/{model_name}.pt\"\n",
        "else:\n",
        "    os.rename(latest, f\"{root_path}/{model_name}.pt\")\n",
        "    latest = f\"{root_path}/{model_name}.pt\"\n",
        "print(latest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAEDRaXIpc3V"
      },
      "source": [
        "# Uploading a pre-trained Model\n",
        "\n",
        "If you have already trained a .pt model using the airockchip/ultralytics_yolo11 repo, then on the right panel of kaggle, press upload in the input section and upload your model.\n",
        "\n",
        "Right click on the uploaded model and copy the path, set `src_path` equal to the copied path in the code cell below before running it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAiqSnL6pc3V"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "src_path = None #r\"/kaggle/input/reefscape-frc-2-yolo11n-200-epochs/pytorch/default/1/Reefscape-FRC-2-640-640-yolo11n.pt\"\n",
        "dst_path = r\"/kaggle/working/\"\n",
        "\n",
        "if src_path != None:\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "    model_name = src_path.split(\"/\")\n",
        "    model_name = model_name[len(model_name) - 1]\n",
        "    model_name = model_name[:-3]\n",
        "\n",
        "    latest = f\"{root_path}/{model_name}.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpcPGzwApc3W"
      },
      "source": [
        "# Validation\n",
        "\n",
        "Arguments for validation here: https://docs.ultralytics.com/modes/val/#arguments-for-yolo-model-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMtnWHxUpc3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b046eabb-7d6e-43ab-889d-217c2deafe07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.9 🚀 Python-3.11.12 torch-2.6.0+cu124 \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/yolo\", line 8, in <module>\n",
            "    sys.exit(entrypoint())\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/ultralytics/ultralytics/cfg/__init__.py\", line 832, in entrypoint\n",
            "    getattr(model, mode)(**overrides)  # default args from model\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ultralytics/ultralytics/engine/model.py\", line 635, in val\n",
            "    validator(model=self.model)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ultralytics/ultralytics/engine/validator.py\", line 127, in __call__\n",
            "    device=select_device(self.args.device, self.args.batch),\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ultralytics/ultralytics/utils/torch_utils.py\", line 192, in select_device\n",
            "    raise ValueError(\n",
            "ValueError: Invalid CUDA 'device=0,1' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\n",
            "\n",
            "torch.cuda.is_available(): True\n",
            "torch.cuda.device_count(): 1\n",
            "os.environ['CUDA_VISIBLE_DEVICES']: None\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!yolo task=detect mode=val model={latest} data={dataset.location}/data.yaml device = cuda:0,1 batch = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rlk0H1upc3X"
      },
      "source": [
        "# RKNN Conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtT08_q5pc3X"
      },
      "source": [
        "## ONNX Conversion\n",
        "\n",
        "Intermediate step between PyTorch models and RKNN models. ONNX is another type of model. ONNX or Open Neural Network Exchange is used so that models can be used across different frameworks, operating systems, and devices.\n",
        "\n",
        "See the Ultralytics YOLO Docs to learn the [arguments](https://docs.ultralytics.com/modes/export/#arguments) and [export formats](https://docs.ultralytics.com/modes/export/#export-formats) of the export function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjs5mNMEpc3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "527151cb-7f9c-44f8-f569-79a8025a50af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ultralytics\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n",
            "Ultralytics 8.3.9 🚀 Python-3.11.12 torch-2.6.0+cu124 CPU (Intel Xeon 2.00GHz)\n",
            "custom_YOLO11n summary (fused): 238 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/Crime_Detection_1-448-448-yolov11n.pt' with input shape (1, 3, 448, 448) BCHW and output shape(s) ((1, 64, 56, 56), (1, 2, 56, 56), (1, 1, 56, 56), (1, 64, 28, 28), (1, 2, 28, 28), (1, 1, 28, 28), (1, 64, 14, 14), (1, 2, 14, 14), (1, 1, 14, 14)) (5.2 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mRKNN:\u001b[0m starting export with torch 2.6.0+cu124...\n",
            "\n",
            "\u001b[34m\u001b[1mRKNN:\u001b[0m feed /content/Crime_Detection_1-448-448-yolov11n.onnx to RKNN-Toolkit or RKNN-Toolkit2 to generate RKNN model.\n",
            "Refer https://github.com/airockchip/rknn_model_zoo/tree/main/examples/\n",
            "\u001b[34m\u001b[1mRKNN:\u001b[0m export success ✅ 0.8s, saved as '/content/Crime_Detection_1-448-448-yolov11n.onnx' (9.9 MB)\n",
            "\n",
            "Export complete (2.4s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Predict:         yolo predict task=detect model=/content/Crime_Detection_1-448-448-yolov11n.onnx imgsz=448  \n",
            "Validate:        yolo val task=detect model=/content/Crime_Detection_1-448-448-yolov11n.onnx imgsz=448 data=/content/ultralytics/Crime-Detection-1/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "💡 Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ],
      "source": [
        "#exports a differently formatted model\n",
        "%cd {root_path}/ultralytics\n",
        "!pip install onnx\n",
        "!yolo mode=export format=rknn model={latest}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZVymsJhpc3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601c9272-d74d-42c8-d568-f13de1476799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Crime_Detection_1-448-448-yolov11n.onnx\n"
          ]
        }
      ],
      "source": [
        "#gets the path of the new onnx model, which is the same as the previous model, best.pt. The new model is called best.onnx\n",
        "ex_path = '.'.join(latest.split('.')[:-1]) + '.onnx'\n",
        "print(ex_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqO5Cdkgpc3Y"
      },
      "source": [
        "## Installing RKNN Toolkit\n",
        "This is the 3rd party toolkit that allows us to convert YOLO11 models from official ultralytics formats to the RKNN format that's used by\n",
        "Rockchips.\n",
        "\n",
        "Check the [RKNN Toolkit 2 ReadMe](https://github.com/rockchip-linux/rknn-toolkit2/blob/master/README.md) to see if it can support your NPU's platform. For example, the RKNN Toolkit 2 can support an Orange Pi 5's NPU Platform: RK3588."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rZ_hXxZrpc3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8df75ee-3404-473b-d306-46391c8d141d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-17 21:36:21--  https://github.com/airockchip/rknn-toolkit2/raw/refs/heads/master/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/airockchip/rknn-toolkit2/refs/heads/master/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl [following]\n",
            "--2025-04-17 21:36:22--  https://raw.githubusercontent.com/airockchip/rknn-toolkit2/refs/heads/master/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39531247 (38M) [application/octet-stream]\n",
            "Saving to: ‘rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl’\n",
            "\n",
            "rknn_toolkit2-2.3.2 100%[===================>]  37.70M   237MB/s    in 0.2s    \n",
            "\n",
            "2025-04-17 21:36:22 (237 MB/s) - ‘rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl’ saved [39531247/39531247]\n",
            "\n",
            "Processing ./rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
            "Collecting protobuf<=4.25.4,>=4.21.6 (from rknn-toolkit2==2.3.2)\n",
            "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: psutil>=5.9.0 in /usr/local/lib/python3.11/dist-packages (from rknn-toolkit2==2.3.2) (5.9.5)\n",
            "Collecting ruamel.yaml>=0.17.21 (from rknn-toolkit2==2.3.2)\n",
            "  Downloading ruamel.yaml-0.18.10-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from rknn-toolkit2==2.3.2) (1.14.1)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from rknn-toolkit2==2.3.2) (4.67.1)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from rknn-toolkit2==2.3.2) (4.11.0.86)\n",
            "Collecting fast-histogram>=0.11 (from rknn-toolkit2==2.3.2)\n",
            "  Downloading fast_histogram-0.14-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting numpy<=1.26.4 (from rknn-toolkit2==2.3.2)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m524.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx>=1.16.1 (from rknn-toolkit2==2.3.2)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime>=1.10.0 (from rknn-toolkit2==2.3.2)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting torch<=2.4.0,>=1.10.1 (from rknn-toolkit2==2.3.2)\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.10.0->rknn-toolkit2==2.3.2)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.10.0->rknn-toolkit2==2.3.2) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.10.0->rknn-toolkit2==2.3.2) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.10.0->rknn-toolkit2==2.3.2) (1.13.1)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.21->rknn-toolkit2==2.3.2)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2)\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2) (12.5.82)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.10.0->rknn-toolkit2==2.3.2)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.4.0,>=1.10.1->rknn-toolkit2==2.3.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.10.0->rknn-toolkit2==2.3.2) (1.3.0)\n",
            "Downloading fast_histogram-0.14-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.10-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, ruamel.yaml.clib, protobuf, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, humanfriendly, ruamel.yaml, onnx, nvidia-cusolver-cu12, nvidia-cudnn-cu12, fast-histogram, coloredlogs, torch, onnxruntime, rknn-toolkit2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.4.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.4 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.4 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 fast-histogram-0.14 humanfriendly-10.0 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 onnx-1.17.0 onnxruntime-1.21.0 protobuf-4.25.4 rknn-toolkit2-2.3.2 ruamel.yaml-0.18.10 ruamel.yaml.clib-0.2.12 torch-2.4.0 triton-3.0.0\n",
            "[Errno 2] No such file or directory: '{root_path}'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/airockchip/rknn-toolkit2/raw/refs/heads/master/rknn-toolkit2/packages/x86_64/rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "!pip install ./rknn_toolkit2-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "\n",
        "%cd {root_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwMBhGW9pc3Z"
      },
      "source": [
        "## Setting Up RKNN Model Zoo 🦓"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH9xfNfopc3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f41ee7-0739-4098-f37e-4db2cc0120d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '{root_path}'\n",
            "/content\n",
            "Cloning into 'rknn_model_zoo'...\n",
            "remote: Enumerating objects: 3202, done.\u001b[K\n",
            "remote: Counting objects: 100% (678/678), done.\u001b[K\n",
            "remote: Compressing objects: 100% (245/245), done.\u001b[K\n",
            "remote: Total 3202 (delta 483), reused 488 (delta 422), pack-reused 2524 (from 2)\u001b[K\n",
            "Receiving objects: 100% (3202/3202), 286.37 MiB | 15.56 MiB/s, done.\n",
            "Resolving deltas: 100% (1094/1094), done.\n"
          ]
        }
      ],
      "source": [
        "%cd {root_path}\n",
        "!git clone https://github.com/airockchip/rknn_model_zoo/\n",
        "%cd rknn_model_zoo\n",
        "%cd examples/yolo11/python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWU9vMiepc3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f494d5b-47b5-4270-9c9c-0263a3f3d533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing imgs.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile imgs.txt\n",
        "imgs/1.jpg\n",
        "imgs/2.jpg\n",
        "imgs/3.jpg\n",
        "imgs/4.jpg\n",
        "imgs/5.jpg\n",
        "imgs/6.jpg\n",
        "imgs/7.jpg\n",
        "imgs/8.jpg\n",
        "imgs/9.jpg\n",
        "imgs/10.jpg\n",
        "imgs/11.jpg\n",
        "imgs/12.jpg\n",
        "imgs/13.jpg\n",
        "imgs/14.jpg\n",
        "imgs/15.jpg\n",
        "imgs/16.jpg\n",
        "imgs/17.jpg\n",
        "imgs/18.jpg\n",
        "imgs/19.jpg\n",
        "imgs/20.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMEQ9VABpc3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a33648-6969-468d-e734-7fe9e1f5f23b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 random images copied from '/content/ultralytics/Crime-Detection-1/test/images' to 'imgs' and renamed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "\n",
        "def copy_and_rename_images(source_folder, destination_folder, n):\n",
        "    if not os.path.exists(source_folder):\n",
        "        print(f\"Source folder '{source_folder}' does not exist.\")\n",
        "        return\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "    image_files = glob.glob(os.path.join(source_folder, '*.jpg'))\n",
        "    selected_images = random.sample(image_files, min(n, len(image_files)))\n",
        "    for i, image_path in enumerate(selected_images, start=1):\n",
        "        destination_path = os.path.join(destination_folder, f'{i}.jpg')\n",
        "        shutil.copy(image_path, destination_path)\n",
        "    print(f\"{min(n, len(image_files))} random images copied from '{source_folder}' to '{destination_folder}' and renamed.\")\n",
        "#putting images from dataset into imgs file\n",
        "copy_and_rename_images(dataset.location+\"/test/images\" , \"imgs\", 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1vAfw5Opc3k"
      },
      "outputs": [],
      "source": [
        "#Goes to the file in line. Everything else in the cell is a list[] of 2 element tuples()\n",
        "@register_line_cell_magic\n",
        "def replaceAllInFile(line, cell):\n",
        "    filename = line.strip()\n",
        "    replacements = eval(cell)  # Assuming input is a valid Python expression\n",
        "    with open(filename, 'r') as f:\n",
        "        file_content = f.read()\n",
        "    for replaced, with_this in replacements: #for every tuple in the list, takes the first element of the tuple in the file, and replaces it with the second element of the tuple\n",
        "        file_content = re.sub(replaced, with_this, file_content)\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(file_content)\n",
        "    print(f\"Replaced successfully in {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suOvs3e0pc3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1264185-4123-4bb4-f8d9-82328f148fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replaced successfully in /content/rknn_model_zoo/examples/yolo11/python/convert.py\n"
          ]
        }
      ],
      "source": [
        "%%replaceAllInFile {root_path}/rknn_model_zoo/examples/yolo11/python/convert.py\n",
        "[\n",
        "    ('../../../datasets/COCO/coco_subset_20.txt', 'imgs.txt'),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LALAu_KCpc3l"
      },
      "source": [
        "## Quantization\n",
        "\n",
        "Here you choose whether to perform quantization, which makes the model lighter and faster, by converting all 32/16 bit floates in the model into 8 bit ints, which costs performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLHh-P2gpc3m"
      },
      "outputs": [],
      "source": [
        "to_quantize = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6upZNXUpc3m"
      },
      "source": [
        "## Exporting to RKNN\n",
        "### You did it!!!! 👏\n",
        "Find the new model which ends in .rknn in the file directories and download it to your laptop. You now possess your own YOLO11 model that can run on an Orange Pi 5.\n",
        "\n",
        "`!python convert.py <ONNX Model> <platform> *optional<dtype> *optional<output_model_path>`\n",
        "\n",
        "* `<onnx_model>`: Specify ONNX model path.\n",
        "* `<TARGET_PLATFORM>`: Specify NPU platform name.\n",
        "  * Platforms: RK3566 | RK3568 | RK3588 | RK3562 | RK3576 | RV1103 | RV1106 | RK1808 | RK3399PRO | RV1109 | RV1126\n",
        "* `<dtype>`: i8/u8 for quantization, fp for no quantization. Default is i8/u8.\n",
        "* `<output_rknn_path>`: Specify save path for the RKNN model, default save in the same directory as ONNX model with name yolo11.rknn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46-ECuPrpc3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1806f2d-30a7-4db6-9c9a-f183c19803dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rknn_model_zoo/examples/yolo11/python\n",
            "RKNN model name: Crime_Detection_1-448-448-yolov11n.rknn\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/rknn_model_zoo/examples/yolo11/python/convert.py\", line 2, in <module>\n",
            "    from rknn.api import RKNN\n",
            "ModuleNotFoundError: No module named 'rknn'\n"
          ]
        }
      ],
      "source": [
        "%cd {root_path}/rknn_model_zoo/examples/yolo11/python\n",
        "quant_code = \"i8\" if to_quantize else \"fp\"\n",
        "image_size = 640\n",
        "\n",
        "#path of outputted rknn model as a string variable\n",
        "output_model = f\"{root_path}/{model_name}.rknn\"\n",
        "print(f\"RKNN model name: {model_name}.rknn\")\n",
        "\n",
        "!python convert.py {ex_path} rk3588 {quant_code} {output_model}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szCaAIwEpc3o"
      },
      "source": [
        "# Deploying Code on PhotonVision for FRC\n",
        "\n",
        "Only works with an Orange Pi 5. Flash a micro SD card with an PhotonVision Orange Pi image using a software like Balena Etcher. Place the micro SD card in the Pi before powering it up. Connect the Pi to your robot's radio using ethernet. Then connect to your robot's radio and type in photonvision.local:5800 in the URL of your search engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB5nNTX5pc3o"
      },
      "source": [
        "### Official PhotonVision release\n",
        "\n",
        "[PhotonVision Docs](https://docs.photonvision.org/en/latest/)\n",
        "\n",
        "Run the code cells below to output a labels.txt file which is needed when uploading your own custom model. You can use PhotonVision's UI to upload your .rknn model and labels.txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTGXuXZlpc3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33462bdb-bfed-4721-d9a1-441c385806f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Successfully created Crime_Detection_1-448-448-yolov11n-labels.txt\n"
          ]
        }
      ],
      "source": [
        "%cd {root_path}\n",
        "\n",
        "labels_file = f\"{model_name}-labels.txt\"\n",
        "\n",
        "import yaml\n",
        "\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream:\n",
        "    data = yaml.safe_load(stream)\n",
        "    with open(f\"{root_path}/{labels_file}\", \"w\") as file:\n",
        "        for index, label in enumerate(data['names']):\n",
        "            if index == len(data['names']) - 1:\n",
        "                file.write(f\"{label}\")\n",
        "            else:\n",
        "                file.write(f\"{label}\\n\")\n",
        "\n",
        "print(f\"Successfully created {labels_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXjDga3_pc3p"
      },
      "source": [
        "### RKNN Fork\n",
        "\n",
        "PhotonVision RKNN Fork Setup Instructions here: https://github.com/laviRZ/photonvision/blob/master/rknn-readme.md."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhG6tPJnpc3q"
      },
      "source": [
        "# YOLO11 Inference\n",
        "You can use [this notebook](https://www.kaggle.com/code/vrishabmakam/frog-force-503-yolo11-inference) to inference a video or image with a PyTorch(.pt) YOLO11 model."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30804,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}