{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30804,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Frog Force 503: YOLO11 Training & RKNN Conversion",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fcascan/YOLO11_Training_-_RKNN_Conversion/blob/main/Frog_Force_503_YOLO11_Training_%26_RKNN_Conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO11 Training and RKNN Conversion\n",
        "### Make a copy of this notebook\n",
        "\n",
        "![frogforce503_logo_2.png](https://frogforce503.org/FFmods/img/frogforce503_logo_2.png)"
      ],
      "metadata": {
        "id": "lpS8sw3PQH8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook trains a YOLO11 model using a Roboflow Dataset and converts it to the RKNN format to run on Rockchip Processors such as the RK3588 found on the Single Board Computer, the Orange Pi 5.\n",
        "\n",
        "To begin, go to session options on the right panel of the editor and select GPU T4 x2 as your accelerator, and enable using the internet.\n",
        "\n",
        "You can also upload your own pre-trained YOLO11 model if you trained it using the airockchip/ultralytics_yolo11 repository rather than training a new model.\n",
        "\n",
        "If you are new to python, object detection, jupyter notebooks, or anything else, check out the helpful links below."
      ],
      "metadata": {
        "id": "cuLgjIdjQH8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpful Links 😀\n",
        "Jupyter Notebook Programming:\n",
        "*   Jupyter - [Jupyter Notebook Documentation](https://jupyter-notebook.readthedocs.io/en/latest/)\n",
        "*   Kaggle - [How to use Kaggle](https://www.kaggle.com/docs/notebooks)\n",
        "*   w3schools - [Python Course](https://www.w3schools.com/python/default.asp)\n",
        "*   Amazon Web Services - [Command Line Interface](https://aws.amazon.com/what-is/cli)\n",
        "*   Tutorialspoint - [Magic Commands](https://www.tutorialspoint.com/jupyter/ipython_magic_commands.htm)\n",
        "\n",
        "\n",
        "Machine Learning Fundamentals:\n",
        "*   IBM - [What is Machine Learning?](https://www.ibm.com/topics/machine-learning)\n",
        "*   IBM - [Neural Networks](https://www.ibm.com/topics/neural-networks)\n",
        "\n",
        "\n",
        "About YOLO11:\n",
        "*   Data Scientist - [What is YOLO?](https://datascientest.com/en/you-only-look-once-yolo-what-is-it)\n",
        "*   Roboflow - [What is YOLO11?](https://blog.roboflow.com/what-is-yolo11)\n",
        "*   Ultralytics - [YOLO Docs](https://docs.ultralytics.com/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "About the Orange Pi 5:\n",
        "\n",
        "*   BAE Systems - [Single Board Computers](https://www.baesystems.com/en-us/definition/what-are-single-board-computers)\n",
        "*   Orange Pi - [Orange Pi 5 Specs](http://www.orangepi.org/html/hardWare/computerAndMicrocontrollers/details/Orange-Pi-5.html)"
      ],
      "metadata": {
        "id": "H2NvlfzmQH8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "YOLO11 is a version of the YOLO(You Only Look Once) computer vision model created by Ultralytics. This notebook will create and train a YOLO11 model from scratch.\n",
        "\n",
        "We are going through a non-standard installation process by cloning the 3rd party ultralytics_yolo11 repository which implements RKNN support in the YOLO11 model.\n",
        "\n",
        "The official ultralytics YOLO11 model does not have RKNN support, so we can't install the standard stuff.\n",
        "\n",
        "This is important because in order for the NPUs the Rockchip Processors to run the model, they have to be formatted in RKNN."
      ],
      "metadata": {
        "id": "B4ig1qv6QH8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os #python module that allows for you to interact with the elements of the operating system like directories, processes, environments, paths\n",
        "\n",
        "#getcwd() gets the working directory we are currently in. We're always working in this directory, we'll store it in avariable called root path\n",
        "#os.getcwd returns a string\n",
        "root_path = os.getcwd()\n",
        "\n",
        "#in a kaggle environment, it should print /kaggle/working\n",
        "print(root_path)\n",
        "\n",
        "#cd stands for change directory, we are just moving into the root path directory\n",
        "#% means we are doing a magic command, % means it works on one line, %% means it works on an entire cell of code. It is used in interactive environments like Jupyter notebooks such as this one.\n",
        "%cd {root_path}\n",
        "\n",
        "#! means a shell command which is in command line interface.\n",
        "!git clone https://github.com/airockchip/ultralytics_yolo11 ultralytics\n",
        "%cd ultralytics\n",
        "\n",
        "#installing dependencies\n",
        "!pip install -e .\n",
        "\n",
        "#import to use for code\n",
        "import ultralytics"
      ],
      "metadata": {
        "trusted": true,
        "id": "diPJOn2BQH8m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading a Dataset\n",
        "[Roboflow Universe](https://universe.roboflow.com/) is an open source repository where you can find datasets to train models you need.\n",
        "\n",
        "With object detection, a dataset is a collection of images that are annotated with bounding boxes and classifications of objects your machine learning model has to detect.\n",
        "\n",
        "For this notebook's use, we want to use a YOLO11 object detection model, that means we need to download a dataset from Roboflow that is formatted to be used by the model.\n",
        "\n",
        "To get a dataset, go to https://universe.roboflow.com/ and find a project. Press datasets on the left bar of the page. Then on the right side, press download dataset. Select a format, for this notebook, select YOLO11 and select show download code. Copy and paste the code snippet in the cell below. You NEED to do this NO MATTER WHAT because each user has their own roboflow API key.\n",
        "\n",
        "Even if you aren't copying a code snippet, you still need to find your API Key. Go to https://app.roboflow.com/ and log in. In the left navigation bar, press settings. Then press API Keys. You will use your private API Key."
      ],
      "metadata": {
        "id": "dsDEBsJ4QH8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"YOUR-API-KEY\")\n",
        "project = rf.workspace(\"robotdetection4003\").project(\"reefscape-frc\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"yolov11\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "F0LwvwcaQH8s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#%cat prints the data in a file (referenced by its path)\n",
        "%cat {dataset.location}/data.yaml"
      ],
      "metadata": {
        "trusted": true,
        "id": "qqPlRUn6QH8u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixing Dataset Directories"
      ],
      "metadata": {
        "id": "jB2p6yXKQH8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream: #open the data.yaml file, 'r' means it is read only\n",
        "    data = yaml.safe_load(stream) #takes the data from yaml and turns it into dictionaries\n",
        "    data[\"train\"] = dataset.location + \"/train/images\" if \"train\" in data else print(\"dataset doesn't have training images\") #the key 'train' contains the path to the train images, replacing existing path with the correct path\n",
        "    data[\"val\"] = dataset.location + \"/valid/images\" if \"val\" in data else print(\"dataset doesn't have validating images\")\n",
        "    data[\"test\"] = dataset.location + \"/test/images\" if \"test\" in data else print(\"dataset doesn't have testing images\")\n",
        "\n",
        "with open(dataset.location + \"/data.yaml\", 'w') as stream: #'w' means we can write in the file\n",
        "    yaml.dump(data, stream, default_flow_style=False) #writes the data variable into the data.yaml file, default flow style just changes how the data looks in the file"
      ],
      "metadata": {
        "trusted": true,
        "id": "-olJE_VzQH8w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%cat {dataset.location}/data.yaml"
      ],
      "metadata": {
        "trusted": true,
        "id": "DV9wKI_VQH8x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Model\n",
        "\n",
        "If you have already trained a model using the airockchip/ultralytics_yolo11 repository, and you want to skip the training steps set the boolean `skip_training` variable equal to True."
      ],
      "metadata": {
        "id": "a7rvL-MzQH8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skip_training = False\n",
        "\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def skip(line, cell):\n",
        "    if not eval(line):\n",
        "        return get_ipython().run_cell(cell)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nEF9UZq6QH8z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup"
      ],
      "metadata": {
        "id": "K43PI4mlQH80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "#register_line_cell_magic means we can write our own magic command\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell): #takes EVERYTHING in the cell and puts it in the file that's on the same line as when write template is called\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))\n",
        "        #** means unspecified number of arguments.\n",
        "        #globals() returns the global symbol table which has the every variable in the notebook.\n",
        "        #Any variables inside the cell are replaced by their values from the global symbol table using the format function.\n",
        "        #write writes the entire formatted cell into the file, including comments\n",
        "        print(\"Wrote successfully to \" + line)"
      ],
      "metadata": {
        "trusted": true,
        "id": "qEQz2qiOQH80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# define number of classes based on YAML\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream: #reads the data.yaml file as the variable stream\n",
        "    #yaml.safeload returns the contents of stream as a dictionary of dictionaries\n",
        "    num_classes = str(len(yaml.safe_load(stream)['names'])) #number of terms in the \"names\" key is the number of classes\n",
        "\n",
        "print(f\"num_classes: {num_classes}\")\n",
        "%cd {root_path}/ultralytics"
      ],
      "metadata": {
        "trusted": true,
        "id": "TTc-78OZQH80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLO11 Architecture\n",
        "Yolo11 architecture found here: https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/models/11/yolo11.yaml.\n",
        "\n",
        "Creating a custom yaml file to fit the number of classes in our dataset so that we can train models from scratch."
      ],
      "metadata": {
        "id": "tprG6YRrQH81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writetemplate custom_yolo11.yaml\n",
        "\n",
        "# Ultralytics YOLO 🚀, AGPL-3.0 license\n",
        "# YOLO11 object detection model with P3-P5 outputs. For Usage examples see https://docs.ultralytics.com/tasks/detect\n",
        "\n",
        "# Parameters\n",
        "nc: {num_classes}  # number of classes\n",
        "scales: # model compound scaling constants, i.e. 'model=yolo11n.yaml' will call yolo11.yaml with scale 'n'\n",
        "  # [depth, width, max_channels]\n",
        "  n: [0.50, 0.25, 1024] # summary: 319 layers, 2624080 parameters, 2624064 gradients, 6.6 GFLOPs\n",
        "  s: [0.50, 0.50, 1024] # summary: 319 layers, 9458752 parameters, 9458736 gradients, 21.7 GFLOPs\n",
        "  m: [0.50, 1.00, 512] # summary: 409 layers, 20114688 parameters, 20114672 gradients, 68.5 GFLOPs\n",
        "  l: [1.00, 1.00, 512] # summary: 631 layers, 25372160 parameters, 25372144 gradients, 87.6 GFLOPs\n",
        "  x: [1.00, 1.50, 512] # summary: 631 layers, 56966176 parameters, 56966160 gradients, 196.0 GFLOPs\n",
        "\n",
        "# YOLO11n backbone\n",
        "backbone:\n",
        "  # [from, repeats, module, args]\n",
        "  - [-1, 1, Conv, [64, 3, 2]] # 0-P1/2\n",
        "  - [-1, 1, Conv, [128, 3, 2]] # 1-P2/4\n",
        "  - [-1, 2, C3k2, [256, False, 0.25]]\n",
        "  - [-1, 1, Conv, [256, 3, 2]] # 3-P3/8\n",
        "  - [-1, 2, C3k2, [512, False, 0.25]]\n",
        "  - [-1, 1, Conv, [512, 3, 2]] # 5-P4/16\n",
        "  - [-1, 2, C3k2, [512, True]]\n",
        "  - [-1, 1, Conv, [1024, 3, 2]] # 7-P5/32\n",
        "  - [-1, 2, C3k2, [1024, True]]\n",
        "  - [-1, 1, SPPF, [1024, 5]] # 9\n",
        "  - [-1, 2, C2PSA, [1024]] # 10\n",
        "\n",
        "# YOLO11n head\n",
        "head:\n",
        "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
        "  - [[-1, 6], 1, Concat, [1]] # cat backbone P4\n",
        "  - [-1, 2, C3k2, [512, False]] # 13\n",
        "\n",
        "  - [-1, 1, nn.Upsample, [None, 2, \"nearest\"]]\n",
        "  - [[-1, 4], 1, Concat, [1]] # cat backbone P3\n",
        "  - [-1, 2, C3k2, [256, False]] # 16 (P3/8-small)\n",
        "\n",
        "  - [-1, 1, Conv, [256, 3, 2]]\n",
        "  - [[-1, 13], 1, Concat, [1]] # cat head P4\n",
        "  - [-1, 2, C3k2, [512, False]] # 19 (P4/16-medium)\n",
        "\n",
        "  - [-1, 1, Conv, [512, 3, 2]]\n",
        "  - [[-1, 10], 1, Concat, [1]] # cat head P5\n",
        "  - [-1, 2, C3k2, [1024, True]] # 22 (P5/32-large)\n",
        "\n",
        "  - [[16, 19, 22], 1, Detect, [nc]] # Detect(P3, P4, P5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cQX0CAFQQH81"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "You can adjust the following settings:\n",
        "\n",
        "1.   model: one of [yolo11n, yolo11s, yolo11m, yolo11l, yolo11x], yolo11n is recommended for a Orange Pi 5.\n",
        "2.   image_size: The input size of the images fed to the model. Should be a multiple of 32.\n",
        "4.   epochs: How many times the model goes through ALL the data\n",
        "\n",
        "View other arguments for the train method here: https://docs.ultralytics.com/modes/train/#train-settings\n",
        "\n",
        "You can see the progress and metrics in the console below the cell. Your cls_loss, box_loss, and dfl_loss should be minimized (less than 1) and your P (precision), R (recall), mAP50, and mAP50-95 should be maximized (as close to 1.00 as possible)."
      ],
      "metadata": {
        "id": "37WvJ7UUQH82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {root_path}/ultralytics\n",
        "\n",
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "image_size = 640\n",
        "model = \"yolo11n\""
      ],
      "metadata": {
        "trusted": true,
        "id": "5WffFNJHQH82"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip skip_training\n",
        "\n",
        "!yolo task=detect mode=train model=./custom_{model}.yaml data={dataset.location}/data.yaml epochs=200 imgsz={image_size} device=cuda:0,1 batch = 200"
      ],
      "metadata": {
        "trusted": true,
        "id": "3vBG44ZBQH84"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Locating trained model"
      ],
      "metadata": {
        "id": "ff8PV-UlQH85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latest_modified_time = 0\n",
        "latest = None\n",
        "\n",
        "#gets the path of the trained model by going through all of folder, subfolders, and files, and finding the file: best.pt\n",
        "for foldername, subfolders, filenames in os.walk(root_path):\n",
        "    for filename in filenames:\n",
        "        if filename == \"best.pt\":\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            modified_time = os.path.getmtime(file_path)\n",
        "            if modified_time > latest_modified_time:\n",
        "                latest_modified_time = modified_time\n",
        "                latest = file_path\n",
        "print(latest)"
      ],
      "metadata": {
        "trusted": true,
        "id": "EKsJWf0hQH85"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Name\n",
        "\n",
        "If you are using an rknn model with PhotonVision for FRC, your model name should be {name}-{horizontal_resolution}-{vertical_resolution}-{model_type}. Name should NOT have dashes, model_type is yolov11n, yolov11s, yolov11m, etc.\n",
        "\n",
        "Modify the `model_name` variable to match your desired name, DO NOT PUT '.pt or .rknn' in the name\n",
        "\n",
        "If you leave `model_name` equal to **None**, still run the code cell below. The code will automatically generate a name that fits model naming criteria PhotonVision's criteria"
      ],
      "metadata": {
        "id": "Tq8hfXatQH86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = None"
      ],
      "metadata": {
        "trusted": true,
        "id": "uP5euTYQQH86"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving and Renaming Model"
      ],
      "metadata": {
        "id": "P2ousT5dQH86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%skip skip_training\n",
        "import os\n",
        "\n",
        "if model_name == None:\n",
        "    dataset_name = dataset.location.replace(f\"{root_path}/ultralytics/\", \"\").replace(\" \", \"_\").replace(\"-\", \"_\")\n",
        "    model_name = f\"{dataset_name}-{image_size}-{image_size}-{model[:4] + 'v' + model[4:]}\" #yolo11n -> yolov11n\n",
        "\n",
        "os.rename(latest, f\"{root_path}/{model_name}.pt\")\n",
        "latest = f\"{root_path}/{model_name}.pt\"\n",
        "print(latest)"
      ],
      "metadata": {
        "trusted": true,
        "id": "SQ7lXDOrQH86"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading a pre-trained Model\n",
        "\n",
        "If you have already trained a .pt model using the airockchip/ultralytics_yolo11 repo, then on the right panel of kaggle, press upload in the input section and upload your model.\n",
        "\n",
        "Right click on the uploaded model and copy the path, set `src_path` equal to the copied path in the code cell below before running it."
      ],
      "metadata": {
        "id": "c6ww_TLzQH87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "src_path = None #r\"/kaggle/input/reefscape-frc-2-yolo11n-200-epochs/pytorch/default/1/Reefscape-FRC-2-640-640-yolo11n.pt\"\n",
        "dst_path = r\"/kaggle/working/\"\n",
        "\n",
        "if src_path != None:\n",
        "    shutil.copy(src_path, dst_path)\n",
        "\n",
        "    model_name = src_path.split(\"/\")\n",
        "    model_name = model_name[len(model_name) - 1]\n",
        "    model_name = model_name[:-3]\n",
        "\n",
        "    latest = f\"{root_path}/{model_name}.pt\""
      ],
      "metadata": {
        "trusted": true,
        "id": "urFIkrZSQH87"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation\n",
        "\n",
        "Arguments for validation here: https://docs.ultralytics.com/modes/val/#arguments-for-yolo-model-validation"
      ],
      "metadata": {
        "id": "Go5vAPpNQH87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=val model={latest} data={dataset.location}/data.yaml device = cuda:0,1 batch = 200"
      ],
      "metadata": {
        "trusted": true,
        "id": "MATYzZfqQH88"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RKNN Conversion"
      ],
      "metadata": {
        "id": "UDvXg_RyQH88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ONNX Conversion\n",
        "\n",
        "Intermediate step between PyTorch models and RKNN models. ONNX is another type of model. ONNX or Open Neural Network Exchange is used so that models can be used across different frameworks, operating systems, and devices.\n",
        "\n",
        "See the Ultralytics YOLO Docs to learn the [arguments](https://docs.ultralytics.com/modes/export/#arguments) and [export formats](https://docs.ultralytics.com/modes/export/#export-formats) of the export function."
      ],
      "metadata": {
        "id": "UF8Q4k8YQH89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#exports a differently formatted model\n",
        "%cd {root_path}/ultralytics\n",
        "!pip install onnx\n",
        "!yolo mode=export format=rknn model={latest}\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9kPbhBbFQH89"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#gets the path of the new onnx model, which is the same as the previous model, best.pt. The new model is called best.onnx\n",
        "ex_path = '.'.join(latest.split('.')[:-1]) + '.onnx'\n",
        "print(ex_path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "SUa6bXxaQH89"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing RKNN Toolkit\n",
        "This is the 3rd party toolkit that allows us to convert YOLO11 models from official ultralytics formats to the RKNN format that's used by\n",
        "Rockchips.\n",
        "\n",
        "Check the [RKNN Toolkit 2 ReadMe](https://github.com/rockchip-linux/rknn-toolkit2/blob/master/README.md) to see if it can support your NPU's platform. For example, the RKNN Toolkit 2 can support an Orange Pi 5's NPU Platform: RK3588."
      ],
      "metadata": {
        "id": "z4vZV_13QH9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/rockchip-linux/rknn-toolkit2/raw/2c2d03def0c0908c86985b8190e973976ecec74c/rknn-toolkit2/packages/rknn_toolkit2-1.6.0+81f21f4d-cp310-cp310-linux_x86_64.whl\n",
        "!pip install ./rknn_toolkit2-1.6.0+81f21f4d-cp310-cp310-linux_x86_64.whl\n",
        "\n",
        "%cd {root_path}"
      ],
      "metadata": {
        "trusted": true,
        "id": "cJNtOziRQH9B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up RKNN Model Zoo 🦓"
      ],
      "metadata": {
        "id": "s4HMo3NEQH9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {root_path}\n",
        "!git clone https://github.com/airockchip/rknn_model_zoo/\n",
        "%cd rknn_model_zoo\n",
        "%cd examples/yolo11/python"
      ],
      "metadata": {
        "trusted": true,
        "id": "GTm_4C8IQH9C"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile imgs.txt\n",
        "imgs/1.jpg\n",
        "imgs/2.jpg\n",
        "imgs/3.jpg\n",
        "imgs/4.jpg\n",
        "imgs/5.jpg\n",
        "imgs/6.jpg\n",
        "imgs/7.jpg\n",
        "imgs/8.jpg\n",
        "imgs/9.jpg\n",
        "imgs/10.jpg\n",
        "imgs/11.jpg\n",
        "imgs/12.jpg\n",
        "imgs/13.jpg\n",
        "imgs/14.jpg\n",
        "imgs/15.jpg\n",
        "imgs/16.jpg\n",
        "imgs/17.jpg\n",
        "imgs/18.jpg\n",
        "imgs/19.jpg\n",
        "imgs/20.jpg"
      ],
      "metadata": {
        "trusted": true,
        "id": "gn56h-Q9QH9I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "\n",
        "def copy_and_rename_images(source_folder, destination_folder, n):\n",
        "    if not os.path.exists(source_folder):\n",
        "        print(f\"Source folder '{source_folder}' does not exist.\")\n",
        "        return\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "    image_files = glob.glob(os.path.join(source_folder, '*.jpg'))\n",
        "    selected_images = random.sample(image_files, min(n, len(image_files)))\n",
        "    for i, image_path in enumerate(selected_images, start=1):\n",
        "        destination_path = os.path.join(destination_folder, f'{i}.jpg')\n",
        "        shutil.copy(image_path, destination_path)\n",
        "    print(f\"{min(n, len(image_files))} random images copied from '{source_folder}' to '{destination_folder}' and renamed.\")\n",
        "#putting images from dataset into imgs file\n",
        "copy_and_rename_images(dataset.location+\"/test/images\" , \"imgs\", 20)"
      ],
      "metadata": {
        "trusted": true,
        "id": "kgVdAzfFQH9J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Goes to the file in line. Everything else in the cell is a list[] of 2 element tuples()\n",
        "@register_line_cell_magic\n",
        "def replaceAllInFile(line, cell):\n",
        "    filename = line.strip()\n",
        "    replacements = eval(cell)  # Assuming input is a valid Python expression\n",
        "    with open(filename, 'r') as f:\n",
        "        file_content = f.read()\n",
        "    for replaced, with_this in replacements: #for every tuple in the list, takes the first element of the tuple in the file, and replaces it with the second element of the tuple\n",
        "        file_content = re.sub(replaced, with_this, file_content)\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(file_content)\n",
        "    print(f\"Replaced successfully in {filename}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "j5G-v3n5QH9J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%replaceAllInFile {root_path}/rknn_model_zoo/examples/yolo11/python/convert.py\n",
        "[\n",
        "    ('../../../datasets/COCO/coco_subset_20.txt', 'imgs.txt'),\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "id": "m2vfVtseQH9K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization\n",
        "\n",
        "Here you choose whether to perform quantization, which makes the model lighter and faster, by converting all 32/16 bit floates in the model into 8 bit ints, which costs performance."
      ],
      "metadata": {
        "id": "ijHtMhWPQH9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_quantize = True"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ikk0aOonQH9K"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting to RKNN\n",
        "### You did it!!!! 👏\n",
        "Find the new model which ends in .rknn in the file directories and download it to your laptop. You now possess your own YOLO11 model that can run on an Orange Pi 5.\n",
        "\n",
        "`!python convert.py <ONNX Model> <platform> *optional<dtype> *optional<output_model_path>`\n",
        "\n",
        "* `<onnx_model>`: Specify ONNX model path.\n",
        "* `<TARGET_PLATFORM>`: Specify NPU platform name.\n",
        "  * Platforms: RK3566 | RK3568 | RK3588 | RK3562 | RK3576 | RV1103 | RV1106 | RK1808 | RK3399PRO | RV1109 | RV1126\n",
        "* `<dtype>`: i8/u8 for quantization, fp for no quantization. Default is i8/u8.\n",
        "* `<output_rknn_path>`: Specify save path for the RKNN model, default save in the same directory as ONNX model with name yolo11.rknn"
      ],
      "metadata": {
        "id": "tjFjFBHsQH9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {root_path}/rknn_model_zoo/examples/yolo11/python\n",
        "quant_code = \"i8\" if to_quantize else \"fp\"\n",
        "image_size = 640\n",
        "\n",
        "#path of outputted rknn model as a string variable\n",
        "output_model = f\"{root_path}/{model_name}.rknn\"\n",
        "print(f\"RKNN model name: {model_name}.rknn\")\n",
        "\n",
        "!python convert.py {ex_path} rk3588 {quant_code} {output_model}"
      ],
      "metadata": {
        "trusted": true,
        "id": "h_QvV8tqQH9L"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploying Code on PhotonVision for FRC\n",
        "\n",
        "Only works with an Orange Pi 5. Flash a micro SD card with an PhotonVision Orange Pi image using a software like Balena Etcher. Place the micro SD card in the Pi before powering it up. Connect the Pi to your robot's radio using ethernet. Then connect to your robot's radio and type in photonvision.local:5800 in the URL of your search engine."
      ],
      "metadata": {
        "id": "gTQ4dmMoQH9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Official PhotonVision release\n",
        "\n",
        "[PhotonVision Docs](https://docs.photonvision.org/en/latest/)\n",
        "\n",
        "Run the code cells below to output a labels.txt file which is needed when uploading your own custom model. You can use PhotonVision's UI to upload your .rknn model and labels.txt file"
      ],
      "metadata": {
        "id": "IxWyqb0zQH9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {root_path}\n",
        "\n",
        "labels_file = f\"{model_name}-labels.txt\"\n",
        "\n",
        "import yaml\n",
        "\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream:\n",
        "    data = yaml.safe_load(stream)\n",
        "    with open(f\"{root_path}/{labels_file}\", \"w\") as file:\n",
        "        for index, label in enumerate(data['names']):\n",
        "            if index == len(data['names']) - 1:\n",
        "                file.write(f\"{label}\")\n",
        "            else:\n",
        "                file.write(f\"{label}\\n\")\n",
        "\n",
        "print(f\"Successfully created {labels_file}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "pBj_DfXAQH9M"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RKNN Fork\n",
        "\n",
        "PhotonVision RKNN Fork Setup Instructions here: https://github.com/laviRZ/photonvision/blob/master/rknn-readme.md."
      ],
      "metadata": {
        "id": "TDHWxFWFQH9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO11 Inference\n",
        "You can use [this notebook](https://www.kaggle.com/code/vrishabmakam/frog-force-503-yolo11-inference) to inference a video or image with a PyTorch(.pt) YOLO11 model."
      ],
      "metadata": {
        "id": "GmUxT07JQH9N"
      }
    }
  ]
}